{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy4sijR4Q2lIHyU9mogRcC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepthiTabithaBennet/NaturalLanguageProcessing/blob/main/NLP_Syntactic_Analysis_POS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plain**"
      ],
      "metadata": {
        "id": "T9vYMqPrpOXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "QHtHYuQLpMCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tagging(text):\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    return pos_tags"
      ],
      "metadata": {
        "id": "qZlq1GyUs5Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tagged_output = pos_tagging(sample_text)\n",
        "\n",
        "for word, tag in tagged_output:\n",
        "    print(word, \":\", tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_W1dJcMrIbD",
        "outputId": "f035a659-7995-4a29-e549-951ac42ab1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The : DT\n",
            "quick : JJ\n",
            "brown : NN\n",
            "fox : NN\n",
            "jumps : VBZ\n",
            "over : IN\n",
            "the : DT\n",
            "lazy : JJ\n",
            "dog : NN\n",
            ". : .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **HMM & Viterbi**"
      ],
      "metadata": {
        "id": "vEzLNKoJpF0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YCVfQsmMFWC",
        "outputId": "295e31ff-4f7d-48aa-a2eb-df1a8289c0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ny69hh9IFtV",
        "outputId": "73d1eddb-1e12-4d37-fc2e-8b86356d8c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged sentence :  [('The', 'NNP'), ('dog', 'NNP'), ('runs', 'NNP'), ('fast', 'NNP')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/tag/hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        }
      ],
      "source": [
        "data = \"Mary Jane can see will. The spot will see Mary. Will Jane spot Mary? Mary will pat Spot\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(data)\n",
        "training_data=[]\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    training_data.append(pos_tags)\n",
        "\n",
        "\n",
        "hmm_tagger = hmm.HiddenMarkovModelTrainer().train(training_data)\n",
        "sentence = [\"The\",\"dog\",\"runs\", \"fast\"]\n",
        "pos_tags = hmm_tagger.tag(sentence)\n",
        "print(\"Tagged sentence : \", pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"Mary Jane can see will. The spot will see Mary. Will Jane spot Mary? Mary will pat Spot\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(data)\n",
        "training_data=[]\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    training_data.append(pos_tags)\n",
        "\n",
        "hmm_tagger = nltk.HiddenMarkovModelTagger.train(training_data)\n",
        "\n",
        "sentence1 = [\"The\",\"dog\",\"runs\", \"fast\"]\n",
        "pos_tags = hmm_tagger.tag(sentence1)\n",
        "print(\"Tagged sentence : \", pos_tags)"
      ],
      "metadata": {
        "id": "TxCixf-LMa_S",
        "outputId": "2891b17d-3995-4c0b-9cf0-c4a3d3abd0b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged sentence :  [('The', 'DT'), ('dog', 'NN'), ('runs', 'MD'), ('fast', 'VB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9Pf9NeGtNKa",
        "outputId": "1b666b98-afcd-4212-b9ce-e055d025b82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.5.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# Sample training data: sequences of observed words and their corresponding POS tags\n",
        "observations = ['the', 'dog', 'barks', 'loudly', 'the', 'cat', 'meows']\n",
        "pos_tags = ['DET', 'NOUN', 'VERB', 'ADV', 'DET', 'NOUN', 'VERB']\n",
        "\n",
        "# Map words and tags to indices\n",
        "word_to_index = {word: i for i, word in enumerate(set(observations))}\n",
        "tag_to_index = {tag: i for i, tag in enumerate(set(pos_tags))}\n",
        "\n",
        "# Convert observations and tags to indices\n",
        "X = np.array([[word_to_index[word]] for word in observations])\n",
        "lengths = [len(observations)]\n",
        "\n",
        "# Create and train the HMM\n",
        "model = hmm.MultinomialHMM(n_components=len(tag_to_index), n_iter=100)\n",
        "model.fit(X, lengths)\n",
        "\n",
        "# Now, let's test the HMM with a new sequence\n",
        "test_sequence = ['the', 'cat', 'barks']\n",
        "X_test = np.array([[word_to_index[word]] for word in test_sequence])\n",
        "\n",
        "# Use the Viterbi algorithm to find the best tag sequence\n",
        "logprob, tag_sequence = model.decode(X_test, algorithm='viterbi')\n",
        "\n",
        "# Convert indices back to tags\n",
        "predicted_tags = [list(tag_to_index.keys())[tag] for tag in tag_sequence]\n",
        "\n",
        "# Print results\n",
        "print(\"Test Sequence:\", test_sequence)\n",
        "print(\"Predicted Tags:\", predicted_tags)"
      ],
      "metadata": {
        "id": "--YJ0Ny3xhBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674fbb4f-a870-42e2-a2ab-9adec529f343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n",
            "WARNING:hmmlearn.base:Fitting a model with 15 free scalar parameters with only 7 data points will result in a degenerate solution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Sequence: ['the', 'cat', 'barks']\n",
            "Predicted Tags: ['DET', 'DET', 'DET']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# Sample training data: sequences of observed words and their corresponding POS tags\n",
        "observations = ['the', 'dog', 'barks', 'loudly', 'the', 'cat', 'meows']\n",
        "pos_tags = ['DET', 'NOUN', 'VERB', 'ADV', 'DET', 'NOUN', 'VERB']\n",
        "\n",
        "# Create mappings from words and tags to indices\n",
        "# This allows us to convert our words and tags into numerical format\n",
        "word_to_index = {word: i for i, word in enumerate(set(observations))}\n",
        "tag_to_index = {tag: i for i, tag in enumerate(set(pos_tags))}\n",
        "\n",
        "# Convert observations to indices\n",
        "# The HMM expects numerical input, so we transform our words into their corresponding indices\n",
        "X = np.array([[word_to_index[word]] for word in observations])\n",
        "\n",
        "# Initialize and train the HMM\n",
        "# We specify the number of components (states) based on unique POS tags\n",
        "model = hmm.MultinomialHMM(n_components=len(tag_to_index), n_iter=100)\n",
        "model.fit(X)\n",
        "\n",
        "# Test sequence of words for prediction\n",
        "test_sequence = ['the', 'cat', 'barks']\n",
        "# Convert the test sequence to indices using the same mapping\n",
        "X_test = np.array([[word_to_index[word]] for word in test_sequence])\n",
        "\n",
        "# Use the Viterbi algorithm to predict the most likely sequence of tags\n",
        "logprob, tag_sequence = model.decode(X_test, algorithm='viterbi')\n",
        "\n",
        "# Convert predicted tag indices back to their corresponding string representations\n",
        "# This uses the tag_to_index mapping to get the original tags\n",
        "predicted_tags = []\n",
        "for tag_index in tag_sequence:\n",
        "    # Find the tag corresponding to the index\n",
        "    predicted_tags.append(list(tag_to_index.keys())[tag_index])\n",
        "\n",
        "# Output the results\n",
        "print(\"Test Sequence:\", test_sequence)\n",
        "print(\"Predicted Tags:\", predicted_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReCg79zFdljV",
        "outputId": "13dcc7f4-7e63-41f7-dec0-eec0714d88cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n",
            "WARNING:hmmlearn.base:Fitting a model with 15 free scalar parameters with only 7 data points will result in a degenerate solution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Sequence: ['the', 'cat', 'barks']\n",
            "Predicted Tags: ['DET', 'NOUN', 'VERB']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# Sample training data: sequences of observed words and their corresponding POS tags\n",
        "observations = ['the', 'dog', 'barks', 'loudly', 'the', 'cat', 'meows']\n",
        "pos_tags = ['DET', 'NOUN', 'VERB', 'ADV', 'DET', 'NOUN', 'VERB']\n",
        "\n",
        "# Create mappings from words and tags to indices\n",
        "word_to_index = {word: i for i, word in enumerate(set(observations))}\n",
        "tag_to_index = {tag: i for i, tag in enumerate(set(pos_tags))}\n",
        "\n",
        "# Convert observations to indices\n",
        "X = np.array([[word_to_index[word]] for word in observations])\n",
        "\n",
        "# Initialize and train the HMM\n",
        "model = hmm.MultinomialHMM(n_components=len(tag_to_index), n_iter=100)\n",
        "model.fit(X)\n",
        "\n",
        "# Display transition and emission probabilities\n",
        "print(\"Transition Matrix:\")\n",
        "print(model.transmat_)  # Transition probabilities\n",
        "\n",
        "print(\"\\nEmission Matrix:\")\n",
        "print(model.emissionprob_)  # Emission probabilities\n",
        "\n",
        "# Test sequence of words for prediction\n",
        "test_sequence = ['the', 'cat', 'barks']\n",
        "X_test = np.array([[word_to_index[word]] for word in test_sequence])\n",
        "\n",
        "# Use the Viterbi algorithm to predict the most likely sequence of tags\n",
        "logprob, tag_sequence = model.decode(X_test, algorithm='viterbi')\n",
        "\n",
        "# Convert predicted tag indices back to their corresponding string representations\n",
        "predicted_tags = [list(tag_to_index.keys())[tag] for tag in tag_sequence]\n",
        "\n",
        "# Output the results\n",
        "print(\"\\nTest Sequence:\", test_sequence)\n",
        "print(\"Predicted Tags:\", predicted_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awBWw6Fbd8S1",
        "outputId": "26d3f065-5622-4a0a-ce71-4d0f599a9901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n",
            "WARNING:hmmlearn.base:Fitting a model with 15 free scalar parameters with only 7 data points will result in a degenerate solution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition Matrix:\n",
            "[[1.56032888e-01 1.12469583e-01 6.66403358e-01 6.50941714e-02]\n",
            " [6.83507168e-02 2.76691546e-01 3.43107588e-01 3.11850149e-01]\n",
            " [7.91015892e-01 2.08342867e-01 6.41001227e-04 2.40274929e-07]\n",
            " [6.63055887e-03 3.60203316e-01 5.95982602e-01 3.71835230e-02]]\n",
            "\n",
            "Emission Matrix:\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "\n",
            "Test Sequence: ['the', 'cat', 'barks']\n",
            "Predicted Tags: ['DET', 'VERB', 'NOUN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pypi.org/project/FastHMM/\n",
        "\n",
        "! pip install FastHMM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntmodufpraty",
        "outputId": "e79a0a8d-30ec-4d8d-b807-cfc1fdfd1889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting FastHMM\n",
            "  Downloading FastHMM-0.1.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading FastHMM-0.1.2-py3-none-any.whl (6.8 kB)\n",
            "Installing collected packages: FastHMM\n",
            "Successfully installed FastHMM-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from FastHMM.hmm import HMMModel\n",
        "from FastHMM.non_rec_viterbi import Viterbi\n",
        "\n",
        "\n",
        "# test model training and predict\n",
        "hmm_model = HMMModel()\n",
        "hmm_model.train_one_line([(\"I\", \"r\"), (\"love\", \"v\"), (\"Beijing\", \"ns\"), (\"Tiananmen\", \"ns\")])\n",
        "hmm_model.train_one_line([(\"you\", \"r\"), (\"go\", \"v\"), (\"Shenzhen\", \"ns\")])\n",
        "result = hmm_model.predict([\"me\", \"love\", \"Guangzhou\"])\n",
        "print(result)\n",
        "\n",
        "# test save and load model\n",
        "hmm_model.save_model()\n",
        "hmm_model = HMMModel().load_model()\n",
        "result = hmm_model.predict([\"we\", \"love\", \"Shenzhen\"])\n",
        "print(result)\n",
        "\n",
        "\n",
        "STATE = ['A', 'B', 'C']\n",
        "\n",
        "PI = {'A': .8}\n",
        "\n",
        "A = {'A': {'A': 0.1,\n",
        "           'B': .7,\n",
        "           'C': .2, },\n",
        "     'B': {'A': .1,\n",
        "           'B': 0.1,\n",
        "           'C': .8},\n",
        "     'C': {'A': .1,\n",
        "           'B': 0.1,\n",
        "           'C': .8}}\n",
        "\n",
        "B = {'A': {'you': 0.5,\n",
        "           'I': 0.5},\n",
        "     'B': {'are': 0.4,\n",
        "           'speak': 0.6},\n",
        "     'C': {'person': 0.5,\n",
        "           'Chinese': 0.5}}\n",
        "\n",
        "viterbi_model = Viterbi(A, B, PI, STATE)\n",
        "trace = viterbi_model.predict_state([\"I\", \"speak\", \"Chinese\"])\n",
        "print(trace)\n",
        "state_sequence = viterbi_model.predict_state([\"I\", \"are\", \"Chinese\"])\n",
        "print(state_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEhd8gJIwWMQ",
        "outputId": "d23e66a4-d151-4c38-e139-f5f655f7a048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('me', 'r'), ('love', 'v'), ('Guangzhou', 'ns')]\n",
            "[('we', 'r'), ('love', 'v'), ('Shenzhen', 'ns')]\n",
            "(['A', 'B', 'C'], 3.9000000000000004)\n",
            "(['A', 'B', 'C'], 3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install hmmlearn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# state space\n",
        "states_space = [\"Sunny\", \"Rainy\", \"Winter\"]\n",
        "no_of_states = len(states_space)\n",
        "print('Number of hidden states :',no_of_states)\n",
        "# observation space\n",
        "observations_space = [\"Dry\", \"Wet\", \"Humid\"]\n",
        "no_of_observations = len(observations_space)\n",
        "print('Number of observations  :',no_of_observations)\n",
        "\n",
        "# state distribution\n",
        "state_probab = np.array([0.5, 0.4, 0.1])\n",
        "print(\"The State probability: \", state_probab)\n",
        "\n",
        "# transition probabilities\n",
        "transition_probab = np.array([[0.2, 0.3, 0.5],\n",
        "                                   [0.3, 0.4, 0.3],\n",
        "                                   [0.5, 0.3, 0.2]])\n",
        "print(\"\\nThe Transition probability:\\n\", transition_probab)\n",
        "# emission probability\n",
        "emission_probab = np.array([[0.2, 0.1, 0.7],\n",
        "                                 [0.2, 0.5, 0.3],\n",
        "                                 [0.4, 0.2, 0.4]])\n",
        "print(\"\\nThe Emission probability:\\n\", emission_probab)\n",
        "\n",
        "model = hmm.CategoricalHMM(n_components=no_of_states)\n",
        "model.startprob_ = state_probab\n",
        "model.transmat_ = transition_probab\n",
        "model.emissionprob_ = emission_probab\n",
        "\n",
        "observations_sequence = np.array([0, 1, 0, 1, 0, 0]).reshape(-1, 1)\n",
        "observations_sequence\n",
        "\n",
        "hidden_states = model.predict(observations_sequence)\n",
        "print(\"The Most likely hidden states are:\", hidden_states)\n",
        "\n",
        "log_probab, hidden_states = model.decode(observations_sequence,\n",
        "                                              lengths = len(observations_sequence),\n",
        "                                              algorithm ='viterbi' )\n",
        "\n",
        "print('Log Probability :',log_probab)\n",
        "print(\"Most likely hidden states:\", hidden_states)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRxAiXbbK-Bs",
        "outputId": "42e65474-91fa-4925-fe17-344946866581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.5.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.5.0)\n",
            "Number of hidden states : 3\n",
            "Number of observations  : 3\n",
            "The State probability:  [0.5 0.4 0.1]\n",
            "\n",
            "The Transition probability:\n",
            " [[0.2 0.3 0.5]\n",
            " [0.3 0.4 0.3]\n",
            " [0.5 0.3 0.2]]\n",
            "\n",
            "The Emission probability:\n",
            " [[0.2 0.1 0.7]\n",
            " [0.2 0.5 0.3]\n",
            " [0.4 0.2 0.4]]\n",
            "The Most likely hidden states are: [1 1 2 1 2 0]\n",
            "Log Probability : -12.575398707022462\n",
            "Most likely hidden states: [1 1 2 1 2 0]\n"
          ]
        }
      ]
    }
  ]
}